[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADRF Onboarding Handbook",
    "section": "",
    "text": "Preface\nThis is a revised version of the Coleridge Initiative ADRF User On-boarding Handbook\nThis is a living document intended to show new ADRF users how to use the platform for common tasks\n© 2023 The Coleridge Initiative, Inc",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "access.html",
    "href": "access.html",
    "title": "1  Obtaining ADRF Access and Account Set Up",
    "section": "",
    "text": "Requesting an Account",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Obtaining ADRF Access and Account Set Up</span>"
    ]
  },
  {
    "objectID": "access.html#requesting-an-account",
    "href": "access.html#requesting-an-account",
    "title": "1  Obtaining ADRF Access and Account Set Up",
    "section": "",
    "text": "Agency-affiliated researcher. If you are an agency-affiliated researcher, your agency will set up an ADRF account for you.\nIndividual part of a training program. If you are part of a training program, Coleridge Initiative will create an account for you once you have been accepted into the program.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Obtaining ADRF Access and Account Set Up</span>"
    ]
  },
  {
    "objectID": "access.html#account-registration-and-onboarding-tasks",
    "href": "access.html#account-registration-and-onboarding-tasks",
    "title": "1  Obtaining ADRF Access and Account Set Up",
    "section": "Account Registration and Onboarding Tasks",
    "text": "Account Registration and Onboarding Tasks\n\nYou will receive an email invitation to activate your account. The email will come from http://okta.com, so please make sure that it doesn’t get caught in your email spam filter. Follow the steps outlined in the email to set up your password and your multi-factor authentication preferences. Clink on the link below to watch a video walking through the steps.\nAfter activating your account, you will be logged in to the ADRF Applications page. Proceed to the Management Portal by clicking on the icon.\nIn the Management Portal, you will notice a “Onboarding Tasks” section within “Admin Tasks” with a number of items you will need to complete before you can gain access to the project space. Refer to the next section for details about the onboarding process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Obtaining ADRF Access and Account Set Up</span>"
    ]
  },
  {
    "objectID": "access.html#obtaining-adrf-access",
    "href": "access.html#obtaining-adrf-access",
    "title": "1  Obtaining ADRF Access and Account Set Up",
    "section": "Obtaining ADRF Access",
    "text": "Obtaining ADRF Access\n\nAgency-affiliated researcher. If you are an agency-affiliated researcher using an agency-sponsored account, you will be granted ADRF access once you complete your onboarding tasks and required data access agreements. If you are a self-paying agency-affiliated researcher, your ADRF access is conditional on receipt of payment. If your institution of Office of Sponsored Programs will be submitting payment on your behalf, please be aware of potential access delays. Whenever possible, the Coleridge Initiative advises paying with a personal credit card or institutional payment card and using the generated invoice to request reimbursement.\nIndividual part of a training program. If you are part of a training program, you will be granted ADRF access once you complete your onboarding tasks and required data access agreements.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Obtaining ADRF Access and Account Set Up</span>"
    ]
  },
  {
    "objectID": "access.html#more-information",
    "href": "access.html#more-information",
    "title": "1  Obtaining ADRF Access and Account Set Up",
    "section": "More Information",
    "text": "More Information\nIf you have any questions, please contact support@coleridgeinitiative.org.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Obtaining ADRF Access and Account Set Up</span>"
    ]
  },
  {
    "objectID": "onboarding.html",
    "href": "onboarding.html",
    "title": "2  Onboarding Modules and Security Training",
    "section": "",
    "text": "Management Portal\nThe Management Portal web-based application is positioned primarily as the management and monitoring console for project and data stewards. It provides detailed insight on project configurations, user activity, user onboarding status, and overall cost of a project on the ADRF. We focus on four primary pillars of information a Project/Data Steward most often focuses on:\nAs mentioned, the Management Portal application will track your ADRF usage. The protal will also consolidate your ADRF Terms of Use, Security Training Quiz, and Security Training Video into one place. In order to complete ADRF onboarding, all three of the mentioned tasks are to be completed by the user (researcher). To access the Management Portal, log in using your credentials at https://adrf.okta.com and click on the ADRF Management Portal icon. See picture below:\nOnce inside the Management Portal, you have access to your personal workspace sessions statistics along with admin tasks such as the three onboarding tasks and password management. See the example below:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Onboarding Modules and Security Training</span>"
    ]
  },
  {
    "objectID": "onboarding.html#management-portal",
    "href": "onboarding.html#management-portal",
    "title": "2  Onboarding Modules and Security Training",
    "section": "",
    "text": "People – Who are the members of projects, how often do they use the ADRF, what exports have they requested and their status, estimated cost per person/project for current month and for the project since inception, and detailed usage metrics.\nProjects – Details of project start/end dates, abstract description, number of members onboarded and pending, and resources the project has access to (i.e. datasets, etc).\nDatasets – Description of the dataset, location on the ADRF (database or file system), size, name of the data steward(s), and the link to Enterprise Data Catalog (Informatica) describing the dataset and metadata.\nAgreements – What agreements are related to these projects, indication of each member’s signing status, members pending signature, and term (dates) covered by the agreement(s).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Onboarding Modules and Security Training</span>"
    ]
  },
  {
    "objectID": "onboarding.html#accessing-the-onboarding-tasks",
    "href": "onboarding.html#accessing-the-onboarding-tasks",
    "title": "2  Onboarding Modules and Security Training",
    "section": "Accessing the Onboarding Tasks",
    "text": "Accessing the Onboarding Tasks\n\nLog in to the Management Portal\nClick on “Admin Tasks” in the left navigation menu.\n\n\n\nClick on “Complete Onboarding”.\n\n\n\nThis will load the Onboarding Tasks window.\n\n\n\nClick on each individual task to complete it.\n\n\nSigning the ADRF Terms of Use Agreement\nThe Terms of Use need to be completed before you are given access to the data and project space inside the ADRF. To complete ADRF Terms of Use, complete the following steps:\n\nClick on the “Terms of Use” tile.\n\n\n\nClick on “Sign with DocuSign”\n\n\n\nYou will then be redirected to the DocuSign signing page. Click “Continue” on the upper right corner.\n\n\n\nClick “Start” to begin.\n\n\n\nIf you have already configured a signature, click on the yellow “Sign” button to apply it. Otherwise, follow the prompts to configure your electronic signature.\n\n\n\nOnce the signature is applied, click “Finish”.\n\n\n\nYou will then be redirected back to the management portal. And the “Terms of Use” task will be marked as completed.\n\n\n\n\nWatching the Security Training Video\nThe Security Training Video needs to be completed as well. To complete the training, complete the following steps:\n\nClick on the “Security Training Video” tile to load the player and then click play.\n\n\n\nOnce you have watched the video in its entirety, click on the “Mark as Complete” button to complete the task.\n\n\n\nNote: the “MARK AS COMPLETE” button will not be enabled until at least 5 minutes have passed since the start of the video.\n\n\nClick on the back arrow in the upper right corner to return to the main tasks panel.\n\n\n\nThe training video section will now be marked as completed.\n\n\n\n\nComplete the Security Training Quiz\nThe Security Training Quiz needs to be completed after the Security Training Video. To complete the training, complete the following steps:\n\nClick on the “Security Quiz” tile to load the quiz.\n\n\n\nAnswer the questions and click on the “SUBMIT RESPONSE” button. You must answer at least four of the questions correctly to complete this task.\n\n\n\nYou will be automatically redirected to the main task panel once the questionnaire has been successfully completed. And the “Security Quiz” will be marked as completed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Onboarding Modules and Security Training</span>"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "3  ADRF - FAQs",
    "section": "",
    "text": "FAQs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ADRF - FAQs</span>"
    ]
  },
  {
    "objectID": "faq.html#faqs",
    "href": "faq.html#faqs",
    "title": "3  ADRF - FAQs",
    "section": "",
    "text": "How do I set up my Multifactor Authentication\n\n\n\n\n\nYou should be prompted to set up multifactor authentication when you create your account, the options are SMS, voice call, email and the Okta verify application.\n\n\n\n\n\n\n\n\n\nCan I set up more than one form of Multifactor Authentication?\n\n\n\n\n\nThis is recommended. If you lose access to one form of MFA, you would still be able to gain access to your account using an alternative. To do so, please log on to https://adrf.okta.com and select your name on the top right and click settings. Here you can modify or set up your SMS, voice call, email or Okta multifactor authentication.\n\n\n\n\n\n\n\n\n\nHow can I reset my Okta password?\n\n\n\n\n\nYou can use the “Need help signing in?” option on the sign on page (https://adrf.okta.com) which will send a link to your email to reset your password. You may have to verify your identify by answering security questions which you set up when creating your account.\n\n\n\n\n\n\n\n\n\nHow can I reset my ADRF password?\n\n\n\n\n\nYou can reset your ADRF project password by following these steps:\n\nClick on the ADRF Management Portal Okta Tile:\n\n\n\nThen click on Admin Tasks on the left hand side of the screen:\n\n\n\nThen click on RESET PASSWORD:\n\n\n\nYou’ll see a screen where you can choose the project(s) for which you want to update the password.\n\n\n\n\n\n\n\n\n\n\nWhat if I do not remember my security questions or if I get locked out?\n\n\n\n\n\nYou would have to reach out to support at support@coleridgeinitiative.org to have your account unlocked and you would have to reset your security questions so that you can recover your account in the future.\n\n\n\n\n\n\n\n\n\nI can log into the ADRF but my desktop and DS application just show blank pages.\n\n\n\n\n\nPlease ensure the connection to ADRF is not being blocked by your organizations VPN and/or firewall (try using a device not connected to your organization’s network) and reach out to support@coleridgeinitiative.org.\n\n\n\n\n\n\n\n\n\nI saved a file in the C: drive or in the Desktop. When I logged back in, the file is no longer there. Can you restore it?\n\n\n\n\n\nThe ADRF is a temporary workspace environment, files left on the desktop will be removed when you log out of your session, and we cannot restore these files. See section 5.2.1 Best practice is to store files in your user folder on the U: drive\n\n\n\n\n\n\n\n\n\nHow do I open an ipynb notebook?\n\n\n\n\n\nOn the desktop you should find an icon for JupyterLab, when you click that, a command prompt and a browser window are opened up, leave the command prompt running. You should be able to open the file by selecting File -&gt; Open From Path  and providing the path to the folder containing the ipynb notebook.\n\n\n\n\n\n\n\n\n\nHow can I ingest publicly available data into the ADRF?\n\n\n\n\n\nPlease open a support request by sending an email to support@coleridgeinitiative.org.  Include the dataset you wish to have available inside the ADRF and documentation that confirms that the dataset is public.\n\n\n\n\n\n\n\n\n\nWhere can I access publicly available data from within the ADRF?\n\n\n\n\n\nPublicly available data is stored in the schema ds_public_1.\n\n\n\n\n\n\n\n\n\nWhere is my project or training related data stored?\n\n\n\n\n\nAll project and training related databases are prefixed with ‘pr_’ (for project) or ‘tr_’ (for training). You may use this space when creating intermediate datasets or as a “working space”. All project members have read and write access to this area (specific to your project).\n\n\n\n\n\n\n\n\n\nMy data is not in a relational format. Where can I find these files?\n\n\n\n\n\nRead-only non-relational data are stored in the G:\\ drive on Windows Explorer. Project specific non-relational data and files are stored in project specific folders that are prefixed with ‘pr_’ or ‘tr_’.  The location of these folders are in the P:\\ drive on Windows Explorer.\n\n\n\n\n\n\n\n\n\nWhat is the difference between the P:, U: and G: drives?\n\n\n\n\n\nEach drive location has a different purpose and access rule:\n          P:  Project specific files shared by ALL project members\n          U:  User personal space.  Only the user has read/write access to this area.\n          G:  Non-relational datasets.  Read-only access to authorized users only.\n\n\n\n\n\n\n\n\n\nI need to process a large amount of relational data. What is the destination location?\n\n\n\n\n\nThe best practice is to process the data where it is currently located.  If the data is in a relational database, perform as much of your processing using Redshift to make the most efficient use of resources (i.e. filtering, sorting, etc).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ADRF - FAQs</span>"
    ]
  },
  {
    "objectID": "dosanddonts.html",
    "href": "dosanddonts.html",
    "title": "4  Do’s and Don’ts For Discussing Data Inside the ADRF",
    "section": "",
    "text": "Exact Numbers\nIt is important to protect the confidential data that is inside the ADRF in communicating with your team-mates. The general rule is that you should never take any exact number out of the ADRF. This means you should never write down or share any number by text, screenshot, or share an image even with a team-mate. The rules have become more complicated now that everything is online, because even though your team-mates are “safe people”, and zoom conversations are password protected and encrypted, we’d rather err on the side of caution when sharing information over zoom.\nThis cheat sheet summarizes some of the rules that apply to discussing data before it has been exported from the ADRF and passed the ADRF team’s disclosure review. If you are unsure about a specific situation, please ask a Coleridge at support@coleridgeinitiative.org.\nDo not describe a statistic in exact numbers. If you would like to communicate these values while not in person, you can have a private discussion via the projects drive inside the ADRF.\nExample: If an average within a specific group was 5,000, you would need to convey this average on the projects drive.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Do's and Don'ts For Discussing Data Inside the ADRF</span>"
    ]
  },
  {
    "objectID": "dosanddonts.html#comparing-values",
    "href": "dosanddonts.html#comparing-values",
    "title": "4  Do’s and Don’ts For Discussing Data Inside the ADRF",
    "section": "Comparing Values",
    "text": "Comparing Values\nWhen comparing values, you are permitted to say that one value is more than, less than, or about the same as another. However, you cannot refer to the exact difference between the two numbers.\nIn practice, you can use pluses and minuses to convey differences between values for data that has not been exported from the ADRF.\nExample: “The mean for Group A was roughly the same as the mean for Group B, but these values were both greater than that of Group C.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Do's and Don'ts For Discussing Data Inside the ADRF</span>"
    ]
  },
  {
    "objectID": "dosanddonts.html#percentagesproportions",
    "href": "dosanddonts.html#percentagesproportions",
    "title": "4  Do’s and Don’ts For Discussing Data Inside the ADRF",
    "section": "Percentages/Proportions",
    "text": "Percentages/Proportions\nPercentages and proportions also cannot be directly mentioned. Instead, you can refer to the percentage/proportion within 25%.\nExample: If a proportion was 30%, you could say “The proportion is about 25%” or “The proportion is between 25% and 50%.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Do's and Don'ts For Discussing Data Inside the ADRF</span>"
    ]
  },
  {
    "objectID": "using.html",
    "href": "using.html",
    "title": "5  Accessing and Using Your Workspace",
    "section": "",
    "text": "Logging into and Logging out of the ADRF\nThis video linked below runs through the necessary steps for logging into and logging out of the ADRF. If the video does not play, click here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and Using Your Workspace</span>"
    ]
  },
  {
    "objectID": "using.html#virtual-desktop-environment",
    "href": "using.html#virtual-desktop-environment",
    "title": "5  Accessing and Using Your Workspace",
    "section": "Virtual Desktop Environment",
    "text": "Virtual Desktop Environment\n\nWhat is a VDE?\nPurpose, Contents, Capabilities\nA virtual desktop environment (VDE) allows you to interact with a remote system as if it were your own personal computer. The majority of your standard desktop functions are available, but the programs, data, and permissions are all controlled by the remote administrator (Coleridge Initiative). Thus, you will be working in a familiar environment while accessing protected data, programs, and systems that would otherwise be difficult to distribute. The ADRF uses a standard Windows environment (Windows Server) and provides a variety of software packages to conduct your analysis. For more on Windows capabilities, see the section on Windows Settings.\n\n\n\n\n\n\n\nTemporary Nature of the Environment\nWhile the environment is similar to that on your home computer (for Windows users), there are a handful of key differences. The first is that the environment is temporary in nature. This means that if you are not using it for a prolonged period of time (default is four hours but can vary by project), running programs will stop running and the information stored in temporary locations will be deleted. You will receive on on-screen message before any sessions are terminated. For more on safe, non-temporary storage locations in the ADRF, see the section on Storing Analytic Results.\nGiven the temporary nature of the ADRF, it is crucial to make sure that your work is saved—and saved in an appropriate location. Once this is complete and you are finished working, make sure that you log out of the ADRF instead of closing the window. To do this, click the rightmost icon on the top taskbar to open up the dropdown menu and select End Session. You will be prompted to double-check that your work is saved prior to ending your session and confirm that you want to end your session.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and Using Your Workspace</span>"
    ]
  },
  {
    "objectID": "using.html#modifying-the-environment",
    "href": "using.html#modifying-the-environment",
    "title": "5  Accessing and Using Your Workspace",
    "section": "Modifying the Environment",
    "text": "Modifying the Environment\n\nEstablishing Personal Folders\nEstablishing your own personal folders is one of the simplest, yet most important, steps to take when setting up your environment. As we note in the section on Storing Analytic Results, the two possible places to store your analytic results or files are in either the U: drive or the P: drive.\nYou will find your personal folder in the U: drive. The folder name will include your Firstname and Lastname, and may additionally include your project workspace number. This is a personal workspace that only you can access in the ADRF.\n\n\nThe U: Drive and the P: Drive\nThe U: drive is your user drive; it’s where you will store any files you are working on. Only the user will have access to the U: drive. For example, if user A wants to share information with user B who is on the same project, user A will need to save files to a P: drive folder and not folders in their U: drive since user B will not be able to access user A’s U: drive.\nThe P: drive is the project drive, which will be used to house project-specific folders. Thus, you and other collaborators on the same project will be able to save files to project drive folders.\nBoth the U: drive and P: drive have defined resource limitations of 150GB. When the workspace exceeds these limits, users will not be able to create new files or save data. The ADRF will not alert users when they approach on 150GB used. Users can check their current usage by right clicking on the user folder and clicking on properties.\n\n\nOther Modifications\nThe top taskbar contains shortcuts to the command prompt, multiple desktop windows, a temporary folder, settings, full-screen view, and toggling multiple monitors.\n\n\n\n\n\n\n\nWindows Settings\nYour desktop will look familiar if you are a Windows user. You will have icons for quick access to programs or browsers on your desktop. The windows icon on the bottom left side of the screen will open up a menu of programs, folders, and other tools, much as you would see on your own desktop. You will have access to PowerShell and several customization settings (e.g., remove bottom taskbar).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and Using Your Workspace</span>"
    ]
  },
  {
    "objectID": "using.html#software-in-the-adrf",
    "href": "using.html#software-in-the-adrf",
    "title": "5  Accessing and Using Your Workspace",
    "section": "Software in the ADRF",
    "text": "Software in the ADRF\n\nJupyterLab\nJupyterLab provides flexible building blocks for interactive, exploratory computing. While JupyterLab has many features found in traditional integrated development environments (IDEs), it remains focused on interactive, exploratory computing. For more on JupyterLab, see the interface documentation.\nThe JupyterLab interface on the ADRF consists of a main work area containing tabs of documents and activities, a collapsible left sidebar, and a menu bar. The left sidebar contains a file browser, the list of running terminals and kernels, the table of contents, and the extension manager.\n\nWhen using Jupyter Notebooks, make sure that all your work is saved to your U: drive and the correct director within the U: drive. You can “nd the active directory by reading the path displayed in the file browser. By default, JupyterLab opens with your U: drive as the base directory. Below, the folder icon in the white box is my user folder (not displayed, but titled Firstname.Lastname; you will have already set up your folder) and subfolder WDQI.\n\n\n\n\n\n\n\nNotebooks\nJupyter Notebooks are documents that combine live runnable code with narrative text (Markdown), equations (LaTeX), images, interactive visualizations, and other rich output. You can create a notebook by clicking the blue + button in the file browser and then selecting a kernel (R, Python3, Stata) in the Launcher tab. For more information on getting started with Jupyter Notebooks, see JupyterLab Notebook documentation.\n\n\nAccessing Stored Data from a Notebook\nA common question is how to access stored data while writing to and using a Jupyter Notebook. Data in the ADRF are stored in a database using Microsoft SQL Server. For more information on how to access stored data in the ADRF based on choice of program (Python, R, Stata), see the section on Accessing Your Data.\n\n\nPython 3\nPython is a general-purpose programming language. You can access Python in a multitude of ways:\n\nThrough the start menu (windows icon). Type in Python. A desktop app called Python 3.7 (64-bit) will populate a window where you can begin programming.\n\n\n\nThrough the command prompt in the top taskbar. Once the command prompt window is open, type in python.\n\n\n\n\n\n\n\nThrough JupyterLab. !is is the recommended way to access Python since it has packages installed and available, and an execution environment for testing and running code (as well as a place to write and save code). Open JupyterLab and make sure your directory is set appropriately in the “le browser. Once there, in your new Launcher window, click the Python 3 icon.\n\n\n\n\n\n\n4. Through Pycharm.\n\n\n\n\n\n\n\nR\nR is a general-purpose programming language. You may access R in one of three ways:\n\nThrough RStudio. This is an integrated development environment (IDE) for R. You can run R code, display variables, debug R code, do inline visualizations, and more. Open RStudio through the desktop shortcut, or type RStudio in the start menu.\nThrough JupyterLab. Open JupyterLab and make sure your directory is set appropriately in the file browser. Once there, in your new Launcher window, click the R icon.\n\n\n\n\n\n\n\nThrough the R GUI (graphical user interface). Type R in the search bar and click to open the RGui.\n\n\n\nStata\nStata is a general-purpose statistical so#ware package. Stata can be accessed through the desktop shortcut StataMP 16 or by searching for it using the search or menu bar, or through JupyterLab.\n\n\nDBeaver\nDBeaver is a universal tool for querying, editing, and managing data stored in Redshift databases. The ADRF stores data using AWS Redshift Server. DBeaver can be accessed through the desktop shortcut DBeaver or by looking it up using the search bar.\nOnce open, you will need to connect to a Redshift server. Please follow the directions in the Redshift Querying Guide Appendix 12.1 section of this guide to connect to the appropriate server.\n\n\nDatabase Navigator\nOn the left side of DBeaver, a pane labeled Database Navigator allows you to easily explore what is in the server to which you are connected. By clicking the arrow, all the items within each server, Database, Schema, etc., are shown. When exploring these data and writing SQL queries, it is frequently useful to have the navigator expanded to see more easily what columns are in each table and their data type; the datatype can be seen in the screenshot to the right in parentheses next to each column name (e.g., clientid(char64) is a text column of length 64— for our purposes you can ignore the char… and varchar… and simply treat it as text).\n\n\n\n\n\n\n\nSQL Editor\nThe SQL Editor is where you can write your own queries to analyze the data. A new script can be opened by clicking on the blue almost-square (looks a bit like an unrolled scroll) on DBeaver’s tool bar:\nThe location of this script button is circled in the red in the upper left of the screenshot below.\n\nNote: If you use the SQL button to open a new window, it will prompt you to select a data source and enter your username and password.\n\n\n\n\n\n\n\n\n\n\n\nOnce you have a SQL Editor window open, you can write a query and run it. One option to run a query is to use the keyboard to hit ctrl+enter, and another option is to use the orange triangle.\n\n\nOpen a saved .sql FIle\nYou do not have to create a new script every time! You can open a .sql file either by simply dragging and dropping it from the file explorer, or by going to File → Open File and navigating to a .sql file, as shown in the screenshot below:\n\n\n\n\n\nOnce you have done so, the top of your SQL Editor window should name the server connection inside the angle brackets to the left of the filename (&lt;Redshift11_projects&gt;).\n\n\n\n\n\n\n\nLibreOffice\nLibreOffice is an office productivity suite. LibreOffice comes equipped with six different programs: a word processor program (Writer), a spreadsheet program (Calc), a presentation program (Impress), a graphics editor program (Draw), a math equation program (Math), and a database management program akin to Microsoft Access (Base). LibreOffice may be accessed through the desktop shortcut DBeaver or by looking it up using the search bar. Once you’ve opened up LibreOffice, you can open any of those six programs, using the left sidebar. For more information on LibreOffice, visit the LibreOffice website.\n\n\n\n\n\nOnce you click on the icon, you’ll see a page with a left sidebar that has a variety of document types under Create. Select the one suited to your needs and double click to open it.\n\n\n\n\n\n\n\nMore…\nThe ADRF provides a number of additional programs such as a simple text editor (Notepad++), PyCharm (an IDE for Python users), and several web browsers. Please note that web browsers are limited only to approved websites.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and Using Your Workspace</span>"
    ]
  },
  {
    "objectID": "using.html#available-software",
    "href": "using.html#available-software",
    "title": "5  Accessing and Using Your Workspace",
    "section": "Available Software",
    "text": "Available Software\nThe ADRF provides numerous software applications to users. Every user in the ADRF has access to:\n\nR Studio\nR\nPython, through Jupyter Labs or PyCharm \nJupyter Labs, R and Python kernels available\nDBeaver\nLibreOffice\nNotepad++ \nMikTex\nJava\n\nIf there is software that you would like to use for your project and is not installed in the ADRF, please email support@coleridgeinitiative.org.\nIf you would like to add additional packages to your workspace, please see the section on Adding Additional Packages in R/Python.\nIf there is a Python/R package you would like installed, please see Additional Packages in R/Python.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and Using Your Workspace</span>"
    ]
  },
  {
    "objectID": "accessing.html",
    "href": "accessing.html",
    "title": "6  Accessing Your Data",
    "section": "",
    "text": "Locate your Data in a Database\nThe ADRF stores data using Redshift. The simplest way to locate and get a quick overview of your data in a database is to use DBeaver. Please see the section on Data Organization, Amazon RedShift Querying Guide to locate and connect to your data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Accessing Your Data</span>"
    ]
  },
  {
    "objectID": "accessing.html#locate-your-data-in-a-database",
    "href": "accessing.html#locate-your-data-in-a-database",
    "title": "6  Accessing Your Data",
    "section": "",
    "text": "G: Drive\nUnstructured data is located on the G: drive inside the file system.\n\n\n\n\n\n\n\nExternal Data and Code\nPlease note that importing of external data and code is restricted to only Coleridge staff. Given the secure and protected environment provided by the ADRF, all code, data, and packages that are coming from outside of the ADRF must be carefully vetted to prevent leaks, disclosure, or unauthorized access. This means that there is no direct method for uploading data or code from your system to the ADRF. Please contact support@coleridgeinitiative.org for any questions or assistance on importing your own code, data, or packages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Accessing Your Data</span>"
    ]
  },
  {
    "objectID": "storing.html",
    "href": "storing.html",
    "title": "7  Storing Analytic Results",
    "section": "",
    "text": "Eligible Locations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing Analytic Results</span>"
    ]
  },
  {
    "objectID": "storing.html#eligible-locations",
    "href": "storing.html#eligible-locations",
    "title": "7  Storing Analytic Results",
    "section": "",
    "text": "User Drive\nThe U: drive is your user drive; it’s where you will store any files you are working on. Only the user will have access to the U: drive. For example, if user A wants to share information with user B who is on the same project, user A will need to save files to a P: drive folder and not folders in their U: drive since user B will not be able to access user A’s U: drive.\n\n\nProject Drive\nThe P: drive also allows permanent storage. This drive is accessible by anyone on the same project, but not across projects. This is the only drive outside of the user drive where saved files will not be erased after logging out of the ADRF.\n\n\nSQL\nEach project will have a project-specific database created. All members of the project will have read and write permissions for data and may also create their own objects (tables, etc.). The project databases are prefixed with pr-.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing Analytic Results</span>"
    ]
  },
  {
    "objectID": "storing.html#ineligible-locations",
    "href": "storing.html#ineligible-locations",
    "title": "7  Storing Analytic Results",
    "section": "Ineligible Locations",
    "text": "Ineligible Locations\nThe G: drive (data), the L: drive (Libs), and the desktop are not eligible for long-term file storing. You won’t have permissions to write to either the G: drive or the L: drive. The desktop will function only as temporary storage—as soon as a user is logged out of the ADRF, your desktop will be cleared. Additionally, since Wi-Fi connectivity can be imperfect, desktop storage for any amount of time is not recommended.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing Analytic Results</span>"
    ]
  },
  {
    "objectID": "storing.html#storage-size-restrictions",
    "href": "storing.html#storage-size-restrictions",
    "title": "7  Storing Analytic Results",
    "section": "Storage Size Restrictions",
    "text": "Storage Size Restrictions\nStorage size varies by project, but is capped at a predetermined amount. Additional storage costs may vary depending on the resource requirements. https://aws.amazon.com/appstream2/pricing/",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing Analytic Results</span>"
    ]
  },
  {
    "objectID": "storing.html#best-practices",
    "href": "storing.html#best-practices",
    "title": "7  Storing Analytic Results",
    "section": "Best Practices",
    "text": "Best Practices\nTo save storage space, try not to save raw data tables—in particular, don’t save copies of or large subsets of data that are already available through standard sources. Instead, access data through the methods described in the prior sections here, as appropriate for your programming language or program.\nOrganize folders in a way that makes sense for your particular project. For example, you might have folders for a particular analysis or sub-projects. Dates on file names can be helpful for version control.\nKeep tabs on how much storage you are using compared to the allocated amount of storage.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Storing Analytic Results</span>"
    ]
  },
  {
    "objectID": "sharing.html",
    "href": "sharing.html",
    "title": "8  Sharing Information within the ADRF",
    "section": "",
    "text": "ADRF Messenger\nThe ADRF messenger is an internal collaboration tool and will be made available once testing is complete.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sharing Information within the ADRF</span>"
    ]
  },
  {
    "objectID": "sharing.html#shared-folders",
    "href": "sharing.html#shared-folders",
    "title": "8  Sharing Information within the ADRF",
    "section": "Shared Folders",
    "text": "Shared Folders\nShared folders within a project are a great way to share information with other members on a team project. Remember that when working with teams you may not share the ADRF screen (even project folders) with other members on video platforms or otherwise, whether or not your team members are working on the same project.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sharing Information within the ADRF</span>"
    ]
  },
  {
    "objectID": "sharing.html#sharing-restrictions",
    "href": "sharing.html#sharing-restrictions",
    "title": "8  Sharing Information within the ADRF",
    "section": "Sharing Restrictions",
    "text": "Sharing Restrictions\nAgain, remember that when working with teams you may not share the ADRF screen with other members on video platforms or otherwise, whether or not your team members are working on the same project.\nThe information contained in the ADRF is restricted to reside only in the ADRF for all purposes unless it passes Export Review. This means that it cannot be shared or potentially shared with any unauthorized parties. Do not write down any numbers or figures or tables corresponding to data in the ADRF. Copying and pasting is restricted, but manually circumventing this is also not permitted by your data agreements.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sharing Information within the ADRF</span>"
    ]
  },
  {
    "objectID": "export.html",
    "href": "export.html",
    "title": "9  Export guidelines",
    "section": "",
    "text": "Export Review Guidelines\nTo provide ADRF users with the ability to draw from sensitive data, results that are exported from the ADRF must meet rigorous standards meant to protect privacy and confidentiality. To ensure that those standards are met, the ADRF Export Review team reviews each request to ensure that it follows formal guidelines that are set by the respective agency providing the data in partnership with the Coleridge Initiative. Prior to moving data into the ADRF from the agency, the Export Review team suggests default guidelines to implement, based on standard statistical approaches in the U.S. government 1,2 as well as international standards 3,4, and 5. The Data Steward from the agency supplying the data works with the team to amend these default rules in line with the agency’s requirements. If you are unsure about the review guidelines for the data you are using in the ADRF or if you have any questions relating to exports, please reach out to support@coleridgeinitiative.org before submitting an export request.\nTo learn more about limiting disclosure more generally, please refer to the textbook or view the videos.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Export guidelines</span>"
    ]
  },
  {
    "objectID": "export.html#export-review-guidelines",
    "href": "export.html#export-review-guidelines",
    "title": "9  Export guidelines",
    "section": "",
    "text": "General Best Practices for a Successful Export\n\nCurrently, the review process is highly manual: Reviewers will read your code and view your output files, which may be time-consuming.\nEach additional release adds disclosure risk and therefore limits subsequent releases; we ask that users limit the number of files they request to export to just the outputs necessary to produce a particular report or paper. If you are requesting an export of more than 10 files, there may be an additional charge.\nThe reviewers may ask you to make changes to your code or output to meet the requirements of guidelines that have been given by the providers of the data in the ADRF. Thus, we strongly encourage you to produce all output files—tables with rounded numbers, graphs with titles, and so forth—through code, rather than manually.\nWe ask that you only request review of final versions of output files, rather than in-progress versions. Any file containing intermediate output will be rejected.\nEvery code file should have a header describing the contents of the file, including a summary of the data manipulation that takes place in the file (e.g., regression, table or figure creation, etc.).\nDocumenting code by using comments throughout is helpful for disclosure reviews. The better the documentation, the faster the turnaround of export requests. If data files are aggregated, please provide documentation on the level of aggregation and for where in the code the aggregation takes place.\nTo help reviewers, who may not have seen your code before, we ask that users create meaningful variable names. For instance, if you are calculating outflows, it is better to name the variable “outflows” than to name it “var1.”\n\n\n\nTimelines for Export Process\n\nColeridge reviewers have five business days to complete an export from the day you submit an export request. However, timelines may differ depending on your agency, so please refer to your specific agency’s guidelines.\nThe review process can be delayed if the reviewer needs additional information or if the reviewer needs you to make changes to your code or output to meet the ADRF nondisclosure requirements.\n\n\n\nExport Review Process\nThe ADRF Export Review process typically involves two main stages:\n\nPrimary Review:\n\nThis is an initial, cursory review of your documentation and exports to ensure they do not include micro-data. A primary review can take up to 5 business days, so please plan accordingly when submitting your materials.\nIn cases where the reviewer has questions or requires additional information, the primary review may extend beyond 5 business days.\n\nSecondary Review:\n\nThis is a comprehensive review conducted by an approved Data Steward who has content knowledge for the data permissioned to your workspace.\nIf your submission pertains to multiple data assets, it will require approval by each Data Steward before the material can be exported from the ADRF.\n\n\nHow to Check Your Export Review Status:\nIf you’ve submitted an export request, you can easily check the status of your submission by following these steps:\n\nLog into the ADRF.\nOpen the ADRF Export module.\n\n\nStatus Descriptions:\nTo help you better understand the different stages of the Export Review process, here are the status descriptions you may encounter:\n\nAwaiting Reviewer:\n\nYour export is currently under primary review. If any issues arise during the primary review, your reviewer will notify you. Upon completion of the primary review, the secondary reviewer(s) will be notified.\n\nAwaiting Secondary Review:\n\nYour export is currently under secondary review. If your submission pertains to multiple data assets, it will require a review by each Data Steward before being approved.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Export guidelines</span>"
    ]
  },
  {
    "objectID": "export.html#preparing-data-for-export",
    "href": "export.html#preparing-data-for-export",
    "title": "9  Export guidelines",
    "section": "Preparing Data for Export",
    "text": "Preparing Data for Export\nEach agency has specific disclosure review guidelines, especially with respect to the minimum allowable cell sizes for tables. Refer to these guidelines when preparing export requests. If you are unsure of what guidelines are in place for the dataset with which you are working in the ADRF, please reach out to support@coleridgeinitiative.org.\n\nTables\n\nCell Sizes\n\nFor individual-level data, please report the number of observations from each cell. For individual-level data, the default rule is to suppress cells with fewer than 10 observations, unless otherwise directed by the guidelines of the agency that provided the data.\nIf your table includes row or column totals or is dependent on a preceding or subsequent table, reviewers will need to take into account complementary disclosure risks—that is, whether the tables’ totals, or the separate tables when read together, might disclose information about individuals in the data in a way that a single, simpler table would not. Reviewers will work with you by offering guidance on implementing any necessary complementary suppression techniques.\n\nWeighted Data\n\nIf weighted results are to be exported, you must report both weighted and unweighted counts.\n\nRatios\n\nIf ratios are reported, please report the number of valid cases for both the numerator and the denominator (e.g., number of men in state X and number of women in state X, in addition to the ratio of women in state X).\n\nPercentiles\n\nDo not report exact percentiles. Instead, for example, you may calculate a “fuzzy median,” by averaging the true 45th and 55th percentiles.\n\nPercentages\n\nFor any reported percentages or proportions, the underlying counts of individuals contributing to the numerators and denominators must be provided for each statistic in the desired export.\n\nMaxima and Minima\n\nSuppress maximum and minimum values in general.\nYou may replace an exact maximum or minimum with a top-coded value.\n\n\n\n\nGraphs\n\nGraphs are representations of tables. Thus, for each graph (which may have, e.g., a jpg, pdf, png, or tif extension), provide the source data of the underlying table of the graph following the guidelines for tables above.\nBecause graphs and other figures take the most time to review, the number of generated graphs should be as low as possible. Please consider the possibility that you could export the underlying table instead, and generate the graph in another package.\nIf a graph is produced from aggregated data or from tables that have been disclosure-proofed following the guidelines above (e.g., bar charts of magnitudes), provide the underlying tables.\nIf a graph is produced directly from unit-record data but aggregated in the visualization (e.g., frequency histograms), provide the underlying tables.\nIf a graph is produced directly from unit-record data and displays unit-record values (e.g., scatterplots, plots of residuals), the graph can be released only after you ensure that individuals cannot be re-identified and that values can only be estimated with a high level of uncertainty. Further processing to meet this requirement can include, but is not restricted to, cutting off the tails of a distribution, removing outliers, jittering the actual values, and removing or modifying axis values.\nIf a graph is produced from the results of modeling or derivation and uses the unit-record data (e.g., regression curves), the graph can be released only if the values cannot be used to find original data values.\n\nGraphs of this type are generally automatically cleared.\nFor precision/recall graphs, you will need to report the sample size used to generate your model(s).\n\n\n\n\nModel Output\n\nOutput from regression or machine-learning models generally does not pose a risk of disclosing personally identifiable information, as long as the models are not based on small samples. Provide the counts for each variable that produces the model output. If categorical variables are used then provide the counts for each category.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Export guidelines</span>"
    ]
  },
  {
    "objectID": "export.html#submitting-an-export-request",
    "href": "export.html#submitting-an-export-request",
    "title": "9  Export guidelines",
    "section": "Submitting an Export Request",
    "text": "Submitting an Export Request\nTo request an export be reviewed, please watch the following video or follow the instructions below:\nExport Video\n\nClick here: http://adrf.okta.com (ADRF 3).\nInput your login credentials.\nVerify yourself with Okta (download Okta Verify on your smartphone or other device).\nChoose your project as seen in the photo below. For the purpose of this document, you are seeing the Coleridge Initiative Associate Access project.\n\n\n\n\n\n\n\nSelect Desktop and login with the same credentials you had done previously.\nUpon entering the ADRF, a chrome page will appear as shown in the photo below. On this page, click Export Request in the bottom left corner. Or, from the ADRF desktop, open Google Chrome and navigate to export.adrf.net. (Note: export.adrf.net is an address that will only work within the ADRF desktop).\n\n\n\n\n\n\n\nClick My Requests, or the top (person-shaped) icon, at the left side of the window as shown in the screenshot below.\n\n\n\n\n\n\n\nClick New Item as shown below\n\n\n\n\n\n\n\nYou will be asked to select the project to which your export relates. If you do not see the correct project listed in the dropdown list, please reach out to our support team at support@coleridgeinitiative.org.\nAfter selecting a project, click Continue.\n\n\n\n\n\n\n\nRead through the entire page that loads. This page, titled “Create Export Request,” will ask for you to comment on all supporting code files to explain the commands used to generate the files in the export request. The Export Review team will reject all requests containing intermediate output. The Export Review team will typically release export requests within five business days. However, if the team has any clarifying questions, this could result in a longer review process. You need to document your output files in the text box provided. See the example below:\n\n\n\n\n\n\n\nWhen you have read through and followed the page instructions, and are ready to proceed:\n\nMove the slider at the bottom of the page to indicate that you have followed the page’s guidelines.\nAt the bottom of the page, upload each of the files that you have prepared.\nClick Submit Request… to create the export request.\n\n\n\n\n\n\nYou can click My Requests at the left side of the window to view your current and previous export requests.\n\nTo learn more about exporting results, please watch these videos.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Export guidelines</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "10  Adding Additional Packages in R/Python",
    "section": "",
    "text": "Adding Additional Packages in R/Python\nThe ADRF has an internal package repository, so users can install packages for R and Python themselves.\nThe repositories that are currently mirrored in the ADRF are CRAN for R packages and PyPi.org for Python. There is currently no access to packages hosted on Github or other mirrors.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding Additional Packages in R/Python</span>"
    ]
  },
  {
    "objectID": "packages.html#adding-additional-packages-in-rpython",
    "href": "packages.html#adding-additional-packages-in-rpython",
    "title": "10  Adding Additional Packages in R/Python",
    "section": "",
    "text": "Note\n\n\n\nIf you are working in a shared workspace for a project, each user in the project must install the packages, there is no shared package installation for projects.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding Additional Packages in R/Python</span>"
    ]
  },
  {
    "objectID": "packages.html#r-packages",
    "href": "packages.html#r-packages",
    "title": "10  Adding Additional Packages in R/Python",
    "section": "R packages",
    "text": "R packages\nTo install R packages, simply type:\ninstall.packages(\"packagename\")\n\nand the package will be installed from the repository. You will not have to re-install the package again, and to use the package load it with the library() function. For example:\nlibrary(tidyverse)\nAll packages will be installed in your user folder.\nTo install a specific package version you can specify:\ninstall.packages(\"remotes\")\nremotes::install_version(\"tidyverse\", \"1.3.2\")\n\n\n\n\n\n\nNote\n\n\n\nWe recommend starting R using Rstudio for best results, instead of double clicking on a R or Rmarkdown script.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding Additional Packages in R/Python</span>"
    ]
  },
  {
    "objectID": "packages.html#python-packages",
    "href": "packages.html#python-packages",
    "title": "10  Adding Additional Packages in R/Python",
    "section": "Python packages",
    "text": "Python packages\nSimilar to R packages, Python packages may be installed using the Package Installer for Python (pip).\n\n\n\n\n\n\nNote\n\n\n\nWe recommend installing python packages from the command line. If you start Jupyter lab, and choose the Terminal tab:\n\nThen install your package using pip, for example, to install the pandas package:\n\nThen you may use the package within your Jupyter notebook as usual.\n\n\nTo install a specific package version type\npip install pandas==1.2.3",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adding Additional Packages in R/Python</span>"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "11  Support",
    "section": "",
    "text": "Technical Support\nFor ADRF technical support, please email support@coleridgeinitiative.org",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Support</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "12  Redshift querying guide",
    "section": "",
    "text": "Introduction\nThis document serves as an introduction to generating proficient Amazon Redshift queries. This is a generalized document meaning you will need to replace “schema_name” and “table_name” with the appropriate schema and table names used for your project.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Redshift querying guide</span>"
    ]
  },
  {
    "objectID": "appendix.html#data-access",
    "href": "appendix.html#data-access",
    "title": "12  Redshift querying guide",
    "section": "Data Access",
    "text": "Data Access\nThe data is housed in Redshift. You need to replace the “user.name.project” with your project based username. The project based username is your user folder name in the U:/ drive:\n\n\n\n\n\n\nNote: Your username will be different than in these examples.\n\nThe password needed to access Redshift is the second password entered when logging into the ADRF as shown in the screen below:\n\n\n\n\n\nAll data is stored under schemas in the projects database and are accessible by the following programs:\n\nDBeaver\nTo establish a connection to Redshift in DBeaver, first double click on the server you wish to connect to. In the example below I’m connecting to Redshift11_projects. Then a window will appear asking for your Username and Password. This will be your user folder name and include adrf\\ before the username. Then click OK. You will now have access to your data stored on the Redshift11_projects server.\n\n\n\n\n\nCreating Tables in PR/TR Schema\nWhen users create tables in their PR (Research Project) or TR (Training Project) schema, the table is initially permissioned to the user only. This is analogous to creating a document or file in your U drive: Only you have access to the newly created table.\nIf you want to allow all individuals in your project workspace to access the table in the PR/TR schema, you will need to grant permission to the table to the rest of the users who have access to the PR or TR schema.\nYou can do this by running the following code:\nGRANT SELECT, UPDATE, DELETE, INSERT ON TABLE schema_name.table_name TO group db_xxxxxx_rw;\n\nNote: In the above code example replace schma_name with the pr_ or tr_ schema assigned to your workspace and replace table_name with the name of the table on which you want to grant access. Also, in the group name db_xxxxxx_rw, replace xxxxxx with your project code. This is the last 6 characters in your project based user name. This will start with either a T or a P.\n\nIf you want to allow only a single user on your project to access the table, you will need to grant permission to that user. You can do this by running the following code:\nGRANT SELECT, UPDATE, DELETE, INSERT ON TABLE schema_name.table_name to \"IAM:first_name.last_name.project_code\";\n\nNote: In the above code example replace schma_name with the pr_ or tr_ schema assigned to your workspace and replace table_name with the name of the table on which you want to grant access. Also, in \"IAM:first_name.last_name.project_code\" update first_name.last_name.project_code with the user name to whom you want to grant access to.\n\nIf you have any questions, please reach out to us at support@coleridgeinitiative.org\nWhen connecting to the database using an ODBC connection, you need to use one of the following DSNs:\n\nRedshift01_projects_DSN\nRedshift11_projects_DSN\n\nIn the code examples below, the default DSN is Redshift01_projects_DSN.\n\n\nSAS Connection\nproc sql;\nconnect to odbc as my con\n(datasrc=Redshift01_projects_DSN user=adrf\\user.name.project password=password);\nselect * from connection to mycon\n(select * form projects.schema.table);\ndisconnect from mycon;\nquit;\n\n\nR Connection\n\nConnecting to Redshift11_projects (Recommended)\n\nNote: You may need to install the packages RJDBC and rstudioapi first.\n\nlibrary(RJDBC)\n\n# Create username\ndbusr=paste(\"ADRF\\\\\", Sys.getenv(\"USERNAME\"), sep= '')                                                                                \n\n# Database URL\nurl &lt;- paste0(\"jdbc:redshift:iam://adrf-redshift11.cdy8ch2udktk.us-gov-west-1.redshift.amazonaws.com:5439/projects;\",\n              \"loginToRp=urn:amazon:webservices:govcloud;\",\n              \"ssl=true;\",\n              \"AutoCreate=true;\",\n              \"idp_host=adfs.adrf.net;\",\n              \"idp_port=443;\",\n              \"ssl_insecure=true;\",\n              \"plugin_name=com.amazon.redshift.plugin.AdfsCredentialsProvider\")\n\n# Redshift JDBC Driver Setting\ndriver &lt;- JDBC(\"com.amazon.redshift.jdbc42.Driver\",\n               classPath = \"C:\\\\drivers\\\\redshift_withsdk\\\\redshift-jdbc42-2.1.0.12\\\\redshift-jdbc42-2.1.0.12.jar\",\n               identifier.quote=\"`\")\ncon &lt;- dbConnect(driver, url, dbusr, rstudioapi::askForPassword())\n\n\nConnecting to Redshift11_projects using .Renviron File\nlibrary(RJDBC)\ndbusr=Sys.getenv(\"DBUSER\")                                                                                  \ndbpswd=Sys.getenv(\"DBPASSWD\")\n\n# Database URL\nurl &lt;- paste0(\"jdbc:redshift:iam://adrf-redshift11.cdy8ch2udktk.us-gov-west-1.redshift.amazonaws.com:5439/projects;\",\n\"loginToRp=urn:amazon:webservices:govcloud;\",\n\"ssl=true;\",\n\"AutoCreate=true;\",\n\"idp_host=adfs.adrf.net;\",\n\"idp_port=443;\",\n\"ssl_insecure=true;\",\n\"plugin_name=com.amazon.redshift.plugin.AdfsCredentialsProvider\")\n\n# Redshift JDBC Driver Setting\ndriver &lt;- JDBC(\"com.amazon.redshift.jdbc42.Driver\",\nclassPath = \"C:\\\\drivers\\\\redshift_withsdk\\\\redshift-jdbc42-2.1.0.12\\\\redshift-jdbc42-2.1.0.12.jar\",\nidentifier.quote=\"`\")\nconn &lt;- dbConnect(driver, url, dbusr, dbpswd)\nFor the above code to work, please create a file name .Renviron in your user folder (user folder is something like i.e. u:\\John.doe.p00002) And .Renviron file should contain the following:\nDBUSER='adrf\\John.doe.p00002'\nDBPASSWD='xxxxxxxxxxxx'\nPLEASE replace user id and password with your project workspace specific user is and password.\nThis will ensure you don’t have your id and password in R code and then you can easily share your R code with others without sharing your ID and password.\n\n\nConnecting to Redshift01_projects (Recommended)\n\nNote: You may need to install the packages RJDBC and rstudioapi first.\n\nlibrary(RJDBC)\n\n# Create username\ndbusr=paste(\"ADRF\\\\\", Sys.getenv(\"USERNAME\"), sep= '')                                                                                \n\n# Database URL\nurl &lt;- paste0(\"jdbc:redshift:iam://adrf-redshift01.cdy8ch2udktk.us-gov-west-1.redshift.amazonaws.com:5439/projects;\",\n              \"loginToRp=urn:amazon:webservices:govcloud;\",\n              \"ssl=true;\",\n              \"AutoCreate=true;\",\n              \"idp_host=adfs.adrf.net;\",\n              \"idp_port=443;\",\n              \"ssl_insecure=true;\",\n              \"plugin_name=com.amazon.redshift.plugin.AdfsCredentialsProvider\")\n\n# Redshift JDBC Driver Setting\ndriver &lt;- JDBC(\"com.amazon.redshift.jdbc42.Driver\",\n               classPath = \"C:\\\\drivers\\\\redshift_withsdk\\\\redshift-jdbc42-2.1.0.12\\\\redshift-jdbc42-2.1.0.12.jar\",\n               identifier.quote=\"`\")\ncon &lt;- dbConnect(driver, url, dbusr, rstudioapi::askForPassword())\n\n\nConnecting to Redshift01_projects using .Renviron File\nlibrary(RJDBC)\ndbusr=Sys.getenv(\"DBUSER\")                                                                    dbpswd=Sys.getenv(\"DBPASSWD\")\n\n# Database URL\nurl &lt;- paste0(\"jdbc:redshift:iam://adrf-redshift01.cdy8ch2udktk.us-gov-west-1.redshift.amazonaws.com:5439/projects;\",\n\"loginToRp=urn:amazon:webservices:govcloud;\",\n\"ssl=true;\",\n\"AutoCreate=true;\",\n\"idp_host=adfs.adrf.net;\",\n\"idp_port=443;\",\n\"ssl_insecure=true;\",\n\"plugin_name=com.amazon.redshift.plugin.AdfsCredentialsProvider\")\n\n# Redshift JDBC Driver Setting\ndriver &lt;- JDBC(\"com.amazon.redshift.jdbc42.Driver\",\nclassPath = \"C:\\\\drivers\\\\redshift_withsdk\\\\redshift-jdbc42-2.1.0.12\\\\redshift-jdbc42-2.1.0.12.jar\",\nidentifier.quote=\"`\")\nconn &lt;- dbConnect(driver, url, dbusr, dbpswd)\nFor the above code to work, please create a file name .Renviron in your user folder (user folder is something like i.e. u:\\John.doe.p00002) And .Renviron file should contain the following:\nDBUSER='adrf\\John.doe.p00002'\nDBPASSWD='xxxxxxxxxxxx'\nPLEASE replace user id and password with your project workspace specific user is and password.\nThis will ensure you don’t have your id and password in R code and then you can easily share your R code with others without sharing your ID and password.\nBest practices for loading large amounts of data in R\n\n\nSQL Basics with R Programming\nTo ensure R can efficiently manage large amounts of data, please add the following lines of code to your R script before any packages are loaded:\noptions(java.parameters = c(\"-XX:+UseConcMarkSweepGC\", \"-Xmx8192m\"))\ngc()\nBest practices for writing tables to Redshift\nWhen writing an R data frame to Redshift use the following code as an example:\n# Note: replace the table_name with the name of the data frame you wish to write to Redshift\n\nDBI::dbWriteTable(conn = conn, #name of the connection \nname = \"schema_name.table_name\", #name of table to save df to \nvalue = df_name, #name of df to write to Redshift \noverwrite = TRUE) #if you want to overwrite a current table, otherwise FALSE\n\nqry &lt;- \"GRANT SELECT ON TABLE schema.table_name TO group &lt;group_name&gt;;\"\ndbSendUpdate(conn,qry)\n\n\n\nPython Connection\nimport pyodbc\nimport pandas as pd\ncnxn = pyodbc.connect('DSN=Redshift01_projects_DSN; UID=adrf\\user.name.project; PWD=password')\ndf = pd.read_sql(\"SELECT * FROM projects.schema_name.table_name\", cnxn)\n\n\nStata Connection\nodbc load, exec(\"select * from PATH_TO_TABLE\") clear dsn(\"Redshift11_projects_DSN\") user(\"adrf\\user.name.project\") password(\"password\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Redshift querying guide</span>"
    ]
  },
  {
    "objectID": "appendix.html#redshift-query-guidelines-for-researchers",
    "href": "appendix.html#redshift-query-guidelines-for-researchers",
    "title": "12  Redshift querying guide",
    "section": "Redshift Query Guidelines for Researchers",
    "text": "Redshift Query Guidelines for Researchers\nDeveloping your query. Here’s an example workflow to follow when developing a query.\n\nStudy the column and table metadata, which is accessible via the table definition. Each table definition can be displayed by clicking on the [+] next the table name.\nTo get a feel for a table’s values, SELECT * from the tables you’re working with and LIMIT your results (Keep the LIMIT applied as you refine your columns) or use (e.g., select * from [table name] LIMIT 1000 )\nNarrow down the columns to the minimal set required to answer your question.\nApply any filters to those columns.\nIf you need to aggregate data, aggregate a small number of rows\nOnce you have a query returning the results you need, look for sections of the query to save as a Common Table Expression (CTE) to encapsulate that logic.\n\n\nDO and DON’T DO BEST PRACTICES:\n\nTip 1: Use SELECT &lt;columns&gt; instead of SELECT *\nSpecify the columns in the SELECT clause instead of using SELECT *. The unnecessary columns place extra load on the database, which slows down not just the single Amazon Redshift, but the whole system.\nInefficient\nSELECT * FROM projects.schema_name.table_name\nThis query fetches all the data stored in the table you choose which might not be required for a particular scenario.\nEfficient\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name\n\n\nTip 2: Always fetch limited data and target accurate results\nLesser the data retrieved, the faster the query will run. Rather than applying too many filters on the client-side, filter the data as much as possible at the server. This limits the data being sent on the wire and you’ll be able to see the results much faster. In Amazon Redshift use LIMIT (###) qualifier at the end of the query to limit records.\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name WHERE [apply some filter] LIMIT 1000\n\n\nTip 3: Use wildcard characters wisely\nWildcard characters can be either used as a prefix or a suffix. Using leading wildcard (%) in combination with an ending wildcard will search all records for a match anywhere within the selected field.\nInefficient\nSelect col_A, col_B, col_C from projects.schema_name.table_name where col_A like '%BRO%'\nThis query will pull the expected results ofBrown Sugar, Brownie, Brown Riceand so on. However, it will also pull unexpected results, such asCountry Brown, Lamb with Broth, Cream of Broccoli.\nEfficient\nSelect col_A, col_B, col_C from projects.schema_name.table_name where col_B like 'BRO%'.\nThis query will pull only the expected results ofBrownie, Brown Rice, Brown Sugar and so on.\n\n\nTip 4: Does My record exist?\nNormally, developers use EXISTS() or COUNT() queries for matching a record entry. However, EXISTS() is more efficient as it will exit as soon as finding a matching record; whereas, COUNT() will scan the entire table even if the record is found in the first row.\nEfficient\nselect col_A from projects.schema_name.table_name A where exists (select 1 from projects.schema_name.table_name B where A.col_A = B.col_A ) order by col_A;\n\n\nTip 5: Avoidcorrelated subqueries\nA correlated subquery depends on the parent or outer query. Since it executes row by row, it decreases the overall speed of the process.\nInefficient\nSELECT col_A, col_B, (SELECT col_C FROM projects.schema_name.table_name_a WHERE col_C = c.rma LIMIT 1) AS new_name FROM projects.schema_name.table_name_b\nHere, the problem is — the inner query is run for each row returned by the outer query. Going over the “table_name_b” table again and again for every row processed by the outer query creates process overhead. Instead, for Amazon Redshift query optimization, use JOIN to solve such problems.\nEfficient\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name c LEFT JOIN projects.schema_name.table_name co ON c.col_A = co.col_B\n\n\nTip 6: Avoid using Amazon Redshift function in the where condition\nOften developers use functions or methods with their Amazon Redshift queries.\nInefficient\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name WHERE RIGHT(birth_date,4) = '1965' and LEFT(birth_date,2) = '07'\nNote that even ifbirth_date has an index, the above query changes the WHERE clause in such a way that this index cannot be used anymore.\nEfficient\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name WHERE birth_date between '711965' and '7311965'\n\n\nTip 7: Use WHERE instead of HAVING\nHAVING clause filters the rows after all the rows are selected. It is just like a filter. Do not use the HAVING clause for any other purposes.It is useful when performing group bys and aggregations.\n\n\nTip 8: Use temp tables when merging large data sets\nCreating local temp tables will limit the number of records in large table joins and merges. Instead of performing large table joins, one can break out the analysis by performing the analysis in two steps: 1) create a temp table with limiting criteria to create a smaller / filtered result set. 2) join the temp table to the second large table to limit the number of records being fetched and to speed up the query. This is especially useful when there are no indexes on the join columns.\nInefficient\nSELECT col_A, col_B, sum(col_C) total FROM projects.schema_name.table_name pd INNER JOIN projects.schema_name.table_name st ON pd.col_A=st.col_B WHERE pd.col_C like 'DOG%' GROUP BY pd.col_A, pd.col_B, pd.col_C\nNote that even if joining column col_A has an index, the col_B column does not. In addition, because the size of some tables can be large, one should limit the size of the join table by first building a smaller filtered #temp table then performing the table joins.\nEfficient\nSET search_path = schema_name; -- this statement sets the default schema/database to projects.schema_name\nStep 1:\nCREATE TEMP TABLE temp_table (\ncol_A varchar(14),\ncol_B varchar(178),\ncol_C varchar(4) );\nStep 2:\nINSERT INTO temp_table SELECT col_A, col_B, col_C\nFROM projects.schema_name.table_name WHERE col_B like 'CAT%';\nStep 3:\nSELECT pd.col_A, pd.col_B, pd.col_C, sum(col_C) as total FROM temp_table pd INNER JOIN projects.schema_name.table_name st ON pd.col_A=st.col_B GROUP BY pd.col_A, pd.col_B, pd.col_C;\nDROP TABLE temp_table;\nNote always drop the temp table after the analysis is complete to release data from physical memory.\n\n\n\nOther Pointers for best database performance\nSELECT columns, not stars. Specify the columns you’d like to include in the results (though it’s fine to use * when first exploring tables — just remember to LIMIT your results).\nAvoid using SELECT DISTINCT. SELECT DISTINCT command in Amazon Redshift used for fetching unique results and remove duplicate rows in the relation. To achieve this task, it basically groups together related rows and then removes them. GROUP BY operation is a costly operation. To fetch distinct rows and remove duplicate rows, use more attributes in the SELECT operation.\nInner joins vs WHERE clause. Use inner join for merging two or more tables rather than using the WHERE clause. WHERE clause creates the CROSS join/ CARTESIAN product for merging tables. CARTESIAN product of two tables takes a lot of time.\nIN versus EXISTS. IN operator is costlier than EXISTS in terms of scans especially when the result of the subquery is a large dataset. We should try to use EXISTS rather than using IN for fetching results with a subquery.\nAvoid\nSELECT col_A , col_B, col_C\nFROM projects.schema_name.table_name\nWHERE col_A IN\n(SELECT col_B FROM projects.schema_name.table_name WHERE col_B = 'DOG')\nPrefer\nSELECT col_A , col_B, col_C\nFROM projects.schema_name.table_name\nWHERE EXISTS\n(SELECT col_A FROM projects.schema_name.table_name b WHERE\na.col_A = b.col_B and b.col_B = 'DOG')\nQuery optimizers can change the order of the following list, but this general lifecycle of a Amazon Redshift query is good to keep in mind when writing Amazon Redshift.\n\nFROM (and JOIN) get(s) the tables referenced in the query.\nWHERE filters data.\nGROUP BY aggregates data.\nHAVING filters out aggregated data that doesn’t meet the criteria.\nSELECT grabs the columns (then deduplicates rows if DISTINCT is invoked).\nUNION merges the selected data into a result set.\nORDER BY sorts the results.\n\n\n\nAmazon Redshift best practices for FROM\nJoin tables using the ON keyword. Although it’s possible to “join” two tables using a WHERE clause, use an explicit JOIN. The JOIN + ON syntax distinguishes joins from WHERE clauses intended to filter the results.\nSET search_path = schema_name;– this statement sets the default schema/database to projects.schema_name\nSELECT A.col_A , B.col_B, B.col_C\nFROM projects.schema_name.table_name as A\nJOIN projects.schema_name.table_name B ON A.col_A = B.col_B\nAlias multiple tables. When querying multiple tables, use aliases, and employ those aliases in your select statement, so the database (and your reader) doesn’t need to parse which column belongs to which table.\nAvoid\nSET search_path = schema_name;– this statement sets the default schema/database to projects.schema_name\nSELECT col_A , col_B, col_C\nFROM dbo.table_name as A\nLEFT JOIN dbo.table_name as B ON A.col_A = B.col_B\nPrefer\nSET search_path = schema_name;– this statement sets the default schema/database to projects.schema_name\nSELECT A.col_A , B.col_B, B.col_C\nFROM dbo.table_name as A\nLEFT JOIN dbo.table_name as B\nA.col_A = B.col_B\n\n\nAmazon Redshift best practices for WHERE\nFilter with WHERE before HAVING. Use a WHERE clause to filter superfluous rows, so you don’t have to compute those values in the first place. Only after removing irrelevant rows, and after aggregating those rows and grouping them, include a HAVING clause to filter out aggregates.\nAvoid functions on columns in WHERE clauses. Using a function on a column in a WHERE clause can really slow down your query, as the function prevents the database from using an index to speed up the query. Instead of using the index to skip to the relevant rows, the function on the column forces the database to run the function on each row of the table. The concatenation operator || is also a function, so don’t try to concat strings to filter multiple columns. Prefer multiple conditions instead:\nAvoid\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name\nWHERE concat(col_A, col_B) = 'REGULARCOFFEE'\nPrefer\nSELECT col_A, col_B, col_C FROM projects.schema_name.table_name\nWHERE col_A ='REGULAR' and col_B = 'COFFEE'\n\n\nAmazon Redshift best practices for GROUP BY\nOrder multiple groupings by descending cardinality. Where possible, GROUP BY columns in order of descending cardinality. That is, group by columns with more unique values first (like IDs or phone numbers) before grouping by columns with fewer distinct values (like state or gender).\n\n\nAmazon Redshift best practices for HAVING\nOnly use HAVING for filtering aggregates. Before HAVING, filter out values using a WHERE clause before aggregating and grouping those values.\nSELECT col_A, sum(col_B) as total_amt\nFROM projects.schema_name.table_name\nWHERE col_C = 1617 and col_A='key'\nGROUP BY col_A\nHAVING sum(col_D)&gt; 0\n\n\nAmazon Redshift best practices for UNION\nPrefer UNION All to UNION. If duplicates are not an issue, UNION ALL won’t discard them, and since UNION ALL isn’t tasked with removing duplicates, the query will be more efficient\n\n\nAmazon Redshift best practices for ORDER BY\nAvoid sorting where possible, especially in subqueries. If you must sort, make sure your subqueries are not needlessly sorting data.\nAvoid\nSELECT col_A, col_B, col_C\nFROM projects.schema_name.table_name\nWHERE col_B IN\n(SELECT col_A FROM projects.schema_name.table_name\nWHERE col_C = 534905 ORDER BY col_B);\nPrefer\nSELECT col_A, col_B, col_C\nFROM projects.schema_name.table_name\nWHERE col_A IN\n(SELECT col_B FROM projects.schema_name.table_name\nWHERE col_C = 534905);\n\n\nTroubleshooting Queries\nThere are several metrics for calculating the cost of the query in terms of storage, time, CPU utilization. However, these metrics require DBA permissions to execute. Follow up with ADRF support to get additional assistance.\nUsing the SVL_QUERY_SUMMARY view: To analyze query summary information by stream, do the following:\nStep 1: select query, elapsed, substring from svl_qlog order by query desc limit 5;\nStep 2: select * from svl_query_summary where query = MyQueryID order by stm, seg, step;\nExecution Plan: Lastly, an execution plan is a detailed step-by-step processing plan used by the optimizer to fetch the rows. It can be enabled in the database using the following procedure:\n\nClick on SQL Editor in the menu bar.\nClick on Explain Execution Plan.\n\nIt helps to analyze the major phases in the execution of a query. We can also find out which part of the execution is taking more time and optimize that sub-part. The execution plan shows which tables were accessed, what index scans were performed for fetching the data. If joins are present it shows how these tables were merged. Further, we can see a more detailed analysis view of each sub-operation performed during query execution.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Redshift querying guide</span>"
    ]
  },
  {
    "objectID": "appendix.html#aws-sources",
    "href": "appendix.html#aws-sources",
    "title": "12  Redshift querying guide",
    "section": "AWS Sources",
    "text": "AWS Sources\nAmazon Redshift best practices for designing queries - Amazon Redshift\nTroubleshooting queries - Amazon Redshift\nAmazon Redshift and PostgreSQL - Amazon Redshift\nUsing the SVL_QUERY_SUMMARY view - Amazon Redshift\nPython:\nExamples of using the Amazon Redshift Python connector - Amazon Redshift\nR:\nExamples of using R with Amazon Redshift\nODBC configuration (for SAS and Stata):\nConfiguring a connection for ODBC driver version 2.x for Amazon Redshift - Amazon Redshift",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Redshift querying guide</span>"
    ]
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "13  Accessing ADRF Dashboards",
    "section": "",
    "text": "1. Setting your dashboard access password\nIf you are a first-time ADRF Users, please follow the instructions in the Onboarding Modules and Security Training to activate your ADRF account and complete your onboarding tasks.\nOnce you have completed the management portal onboarding tasks, you will next need to set your dashboard access password. This is separate from the first password you use to access the ADRF through Okta, and will instead be used to provide specific access to the dashboard. You should only need to do this the first time you access the dashboard, but you can always follow these instructions if you need to update or reset your dashboard access password in the future.\nIn the Management Portal, again navigate to the “Admin Tasks” page by clicking the link on the sidebar navigation menu:\nClick on the “Reset Password” button:\nThis will load the password reset window:\nSelect the account associated with the dashboard by clicking on the checkbox on the right:\nEnter the desired password. The chosen password must adhere to the ADRF password policy:\nClick the “Reset Password” button to proceed with the update. You will receive confirmation at the bottom of the window once the password has been successfully updated:\nPlease email support@coleridgeinitiative.org if you have any issues resetting this password.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Accessing ADRF Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#setting-your-dashboard-access-password",
    "href": "dashboards.html#setting-your-dashboard-access-password",
    "title": "13  Accessing ADRF Dashboards",
    "section": "",
    "text": "Important: Take note of the username associated with your dashboard (John.Doe.P00000 in this example). You will need to enter this username again in the next step. This is also the user name referenced in your onboarding email.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Accessing ADRF Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#accessing-the-dashboard",
    "href": "dashboards.html#accessing-the-dashboard",
    "title": "13  Accessing ADRF Dashboards",
    "section": "2. Accessing the Dashboard",
    "text": "2. Accessing the Dashboard\nOnce you have successfully reset your dashboard access password, you are ready to access the dashboard. To do so, navigate back to the main Okta portal (adrf.okta.com) and click on the tile associated with your dashboard. This tile will be unavailable until you complete the three ADRF onboarding tasks discussed in Step 1:\n\nClicking on this will bring up another window where you will be prompted to “Choose Your Application to Get Started.” Click on your Dashboard icon:\n\nNext, you will need to wait for your session to be prepared. Then, your session will load the secure browser window, which will then bring up the Posit Connect portal. The Posit Connect portal is used to host the Dashboard. This step may take several seconds while the browser loads and prepares the dashboard data.\nBefore accessing the dashboard, you will then be presented with one final log in, to the secure Connect environment:\n\nHere, please enter:\n\nThe username you saw in the Password Reset step above (e.g., John.Doe.P00000)\nYour dashboard access password that you set in Step 2.\n\n\nOnce you enter the appropriate information and click “Log In,” your dashboard should begin to load. This again may take a minute or two - if you run into any issues, please let us know!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Accessing ADRF Dashboards</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Confidential Information Protection and Statistical Efficiency Act of 2002. (Washington, DC: U.S. GPO, 2002).\nFederal Committee on Statistical Methodology. “Report on Statistical Disclosure Limitation Methodology,” 22 (Second Version, 2005). https://nces.ed.gov/fcsm/pdf/spwp22.pdf.\n“How to Use Microdata Properly: Self-Study Material for the Users of Eurostat Microdata Sets.” (2018). https://ec.europa.eu/eurostat/web/microdata/overview/self-study-material-for-microdata-users.\nResearch Data Centre of the German Federal Employment Agency at the Institute for Employment Research. “Remote Data Access and On-Site Use at the FDZ of the BA at the IAB.” (2020, December 8). http://doku.iab.de/fdz/access/Vorgaben_DAFE_EN.PDF\nWelpton, Richard. Handbook on Statistical Disclosure Control for Outputs. (figshare, 2019). https://doi.org/10.6084/m9.figshare.9958520.v1.",
    "crumbs": [
      "References"
    ]
  }
]